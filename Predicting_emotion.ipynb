{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6cc7ec-141f-4a30-abc7-ca966f78174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ad20f-28be-4283-bd54-9bce68092a4b",
   "metadata": {},
   "source": [
    "Loading our training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5821f5d8-7577-4e23-9ed7-8173030e70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=6720, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_emotion_model import EmotionCNN\n",
    "model = EmotionCNN(numclasses=8)\n",
    "model.load_state_dict(torch.load(\"emotion_cnn.pth\",weights_only=True))  #loading saved weights\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11cbf5-f5c6-4de8-be9c-e7479c5ca585",
   "metadata": {},
   "source": [
    "Defining feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d75de5b-4e1f-4b21-849b-4e54ea9d79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path,max_len=174):\n",
    "    # Load audio file\n",
    "    audio, sample_rate = librosa.load(file_path, sr=22050)\n",
    "    # Extract 40 MFCC features\n",
    "    mfcc=librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    # Normalize the MFCCs\n",
    "    mfcc=(mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf47b2-3b7e-4b8b-b792-46ac35115174",
   "metadata": {},
   "source": [
    "Preparing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd42bc9-da31-49ec-82d3-5b0d2bc87f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"Emotionalspeech/Actor_04/03-01-03-01-02-01-04.wav\" #new test audio file\n",
    "mfcc = extract_features(file_path)\n",
    "input_tensor=torch.tensor(mfcc).unsqueeze(0).unsqueeze(0).float()#Reshape to (1,1,40,time_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6866e-658c-4586-b5d2-63a74cba433a",
   "metadata": {},
   "source": [
    "Predicting output and decoding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e25574-9751-4c8e-9147-47e10d35ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: happy\n"
     ]
    }
   ],
   "source": [
    "output = model(input_tensor)\n",
    "predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(['angry','calm','disgust','fearful','happy','neutral','sad','surprised'])\n",
    "\n",
    "print(\"Predicted Emotion:\", le.inverse_transform([predicted_label])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-env]",
   "language": "python",
   "name": "conda-env-torch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
